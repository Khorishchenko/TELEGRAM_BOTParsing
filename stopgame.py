from hashlib import new
import re
import os.path
import requests
from bs4 import BeautifulSoup as BS
from urllib.parse import urlparse

class StopGame:
	host = 'https://stopgame.ru'
	url = 'https://stopgame.ru/review/new'
	lastkey = ""
	lastkey_file = ""

	def __init__(self, lastkey_file):
		self.lastkey_file = lastkey_file

		if(os.path.exists(lastkey_file)):
			self.lastkey = open(lastkey_file, 'r').read()
		else:
			f = open(lastkey_file, 'w')
			self.lastkey = self.get_lastkey()
			f.write(self.lastkey)
			f.close()

	# def new_games(self):
	# 	r = requests.get(self.url)
	# 	html = BS(r.content, 'html.parser')

	# 	new = []
	# 	items = html.select('.tiles > .items > .item > a')
	# 	for i in items:
	# 		key = self.parse_href(i['href'])
	# 		if(self.lastkey < key):
	# 			new.append(i['href'])

	# 	return new
 
	# def new_games(self):
	# 	r = requests.get(self.url)
	# 	html = BS(r.content, 'html.parser')

	# 	new = []
	# 	items = html.select('.tiles > .items > .item > a')
	# 	for i in items:
	# 		new.append(i['href'])
		
	# 	return new
	
	def new_games(self):
		r = requests.get(self.url)
		if r.status_code == 200:
			html = BS(r.content, 'html.parser')
			print("Connection successful.")
		else:
			print(f"Error: Connection failed with status code {r.status_code}.")
  
		games = []
		items = html.select('.list-view > ._default-grid_1mwhk_258 > div > article > a')
	
		if items:
			print("Elements found.") 
		else:
			print("No elements found.")
        
		for item in items:
			href = item['href']
			games.append(href)

		return games

	def game_info(self, uri):
		link = self.host + uri
		r = requests.get(link)
		html = BS(r.content, 'html.parser')

		# –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
  
		# –®—É–∫–∞—î–º–æ —Ç–µ–≥ <source>
		source_tag = html.find('source')
		# –Ø–∫—â–æ —Ç–µ–≥ <source> –∑–Ω–∞–π–¥–µ–Ω–æ
		if source_tag:
			# –û—Ç—Ä–∏–º—É—î–º–æ URL –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –∞—Ç—Ä–∏–±—É—Ç–∞ srcset
			image_url = source_tag['srcset'].split(',')[0].strip().split()[0]

		# –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
		title = html.find('h1').text.strip()

		# –û—Ç—Ä–∏–º–∞–Ω–Ω—è –æ–ø–∏—Å—É
		description = html.find('p', class_='_text_12po9_111 _text-width_12po9_111').text.strip()

		# –û—Ç—Ä–∏–º–∞–Ω–Ω—è –æ—Ü—ñ–Ω–∫–∏
		rating_container = html.find('section', class_='_news-games-container_12po9_1602')

		if rating_container:
			rating_icons = rating_container.find('svg', class_='_sg-rating_17n8v_478')

		# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä—è–¥–∫–∞ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
		score = ""
		if rating_icons.find('use')['href'] == '#ratings/izum':
			score = "‚≠ê‚≠ê‚≠ê‚≠ê"  
		if rating_icons.find('use')['href'] == '#ratings/pohvalno':
			score = "‚≠ê‚≠ê‚≠ê"  
		if rating_icons.find('use')['href'] == '#ratings/prohodnyak':
			score = "‚≠ê‚≠ê"  
		if rating_icons.find('use')['href'] == '#ratings/musor':
			score = "‚≠ê"  		
		else:
			score = "–û—Ü—ñ–Ω–∫–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞"  # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —è–∫—â–æ –æ—Ü—ñ–Ω–∫–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞	

		info = {
				"link": link, 
				"image": image_url,
				"title": title,
				"score": score,
				"excerpt": description
			}
		
		return info

	def download_image(self, url):
		r = requests.get(url, allow_redirects=True)

		a = urlparse(url)
		filename = os.path.basename(a.path)
		open(filename, 'wb').write(r.content)

		return filename

	def identify_score(self, score):
		if(score == 'score-1'):
			return "–ú—É—Å–æ—Ä üëé"
		elif(score == 'score-2'):
			return "–ü—Ä–æ—Ö–æ–¥–Ω—è–∫ ‚úã"
		elif(score == 'score-3'):
			return "–ü–æ—Ö–≤–∞–ª—å–Ω–æ üëç"
		elif(score == 'score-4'):
			return "–ò–∑—É–º–∏—Ç–µ–ª—å–Ω–æ üëå"

	def get_lastkey(self):
		r = requests.get(self.url)
		html = BS(r.content, 'html.parser')

		items = html.select('.list-view > ._default-grid_1mwhk_258 > div > article > a')
		return self.parse_href(items[0]['href'])

	def parse_href(self, href):
		return href.split('/')[-1]

	def update_lastkey(self, new_key):
		self.lastkey = new_key

		with open(self.lastkey_file, "r+") as f:
			data = f.read()
			f.seek(0)
			f.write(str(new_key))
			f.truncate()

		return new_key
	
	def parse_div(self):
		r = requests.get(self.url)
		html_content = r.content
		soup = BS(html_content, 'html.parser')

		# –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ <div>, —è–∫—ñ –º–∞—é—Ç—å –∞—Ç—Ä–∏–±—É—Ç data-key
		divs_with_data_key = soup.find_all('div', attrs={'data-key': True})

		new = []
		for div in divs_with_data_key:
			article = div.find('article')
			new.append(article['aria-label'])

		return new